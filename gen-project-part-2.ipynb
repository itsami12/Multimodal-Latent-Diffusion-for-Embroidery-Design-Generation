{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14045022,"sourceType":"datasetVersion","datasetId":8941399},{"sourceId":14045030,"sourceType":"datasetVersion","datasetId":8941406}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# basic_pixel_diffusion_kaggle.py\n# Hard-coded dataset + captions for your Kaggle paths.\nimport os, math, random, json\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, utils\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# --------------------------------------\n# HARD-CODED CONFIG (uses your Kaggle paths)\n# --------------------------------------\nCONFIG = {\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"img_size\": 256,\n    \"batch_size\": 8,\n    \"epochs\": 20,\n    \"lr\": 2e-4,\n    \"timesteps\": 1000,\n    \"sample_steps\": 50,\n    \"guidance_scale\": 5.0,\n    \"cf_prob\": 0.1,\n    \"save_dir\": \"./basic_diff_ckpts\",\n    \"val_fraction\": 0.1,\n    # --- YOUR PATHS (hard-coded) ---\n    \"data_root\": \"/kaggle/input/combineed/kaggle/working/dataset_combined\",\n    \"captions_json_folder\": \"/kaggle/input/jssonfile/kaggle/working/caption_jsons_multiGPU\",\n}\nos.makedirs(CONFIG[\"save_dir\"], exist_ok=True)\nos.makedirs(f\"{CONFIG['save_dir']}/samples\", exist_ok=True)\ntorch.manual_seed(42); random.seed(42); np.random.seed(42)\n\n# --------------------------------------\n# Utilities\n# --------------------------------------\ndef save_image_grid(tensor, path, nrow=4):\n    grid = utils.make_grid((tensor.clamp(-1,1)+1)/2.0, nrow=nrow)\n    arr = grid.mul(255).permute(1,2,0).cpu().numpy().astype(\"uint8\")\n    Image.fromarray(arr).save(path)\n\ndef plot_and_save(history, save_dir):\n    epochs = range(1, len(history['train_loss'])+1)\n    plt.figure(figsize=(8,4))\n    plt.plot(epochs, history['train_loss'], label='train_loss')\n    if history.get('val_loss') is not None:\n        plt.plot(epochs, history['val_loss'], label='val_loss')\n    plt.xlabel('epoch'); plt.ylabel('MSE loss'); plt.legend(); plt.title('Loss')\n    plt.tight_layout(); plt.savefig(os.path.join(save_dir, 'loss_curve.png')); plt.close()\n\n    plt.figure(figsize=(8,4))\n    plt.plot(epochs, history['train_acc'], label='train_acc')\n    if history.get('val_acc') is not None:\n        plt.plot(epochs, history['val_acc'], label='val_acc')\n    plt.xlabel('epoch'); plt.ylabel('accuracy'); plt.legend(); plt.title('Accuracy')\n    plt.tight_layout(); plt.savefig(os.path.join(save_dir, 'acc_curve.png')); plt.close()\n    print(\"Saved plots to\", save_dir)\n\n# --------------------------------------\n# Caption JSON loader (robust minimal)\n# --------------------------------------\ndef load_caption_map(json_folder: str):\n    caption_map = {}\n    if json_folder is None:\n        return caption_map\n    jfolder = Path(json_folder)\n    if not jfolder.exists():\n        print(\"Caption folder not found:\", json_folder)\n        return caption_map\n    files = []\n    if jfolder.is_file():\n        files = [jfolder]\n    else:\n        files = [p for p in jfolder.rglob(\"*\") if p.suffix.lower() in ('.json','.ndjson','.jsonl')]\n    for jf in files:\n        try:\n            data = json.load(open(jf, 'r'))\n            if isinstance(data, list):\n                for obj in data:\n                    fn = obj.get('image_filename') or (obj.get('image_path') and Path(obj.get('image_path')).name)\n                    cap = obj.get('caption') or obj.get('text') or \" \"\n                    if fn:\n                        caption_map[Path(fn).name] = cap\n            elif isinstance(data, dict):\n                # either mapping filename->caption or single object with keys\n                if 'image_filename' in data and 'caption' in data:\n                    caption_map[Path(data['image_filename']).name] = data.get('caption',' ')\n                else:\n                    for k, v in data.items():\n                        if isinstance(v, str):\n                            caption_map[Path(k).name] = v\n                        elif isinstance(v, dict) and 'caption' in v:\n                            caption_map[Path(k).name] = v['caption']\n        except Exception:\n            # try line-delimited json\n            try:\n                with open(jf, 'r') as fh:\n                    for line in fh:\n                        line = line.strip()\n                        if not line: continue\n                        obj = json.loads(line)\n                        fn = obj.get('image_filename') or (obj.get('image_path') and Path(obj.get('image_path')).name)\n                        cap = obj.get('caption') or obj.get('text') or \" \"\n                        if fn: caption_map[Path(fn).name] = cap\n            except Exception:\n                print(\"Warning: could not parse\", jf)\n                continue\n    print(f\"Loaded {len(caption_map)} captions from {json_folder}\")\n    return caption_map\n\n# --------------------------------------\n# Dataset using captions\n# --------------------------------------\nclass SimpleImageDatasetWithCaptions(Dataset):\n    def __init__(self, root, captions_map=None, img_size=256):\n        self.root = Path(root)\n        self.files = sorted([p for p in self.root.rglob(\"*\") if p.suffix.lower() in (\".jpg\",\".jpeg\",\".png\")])\n        self.caption_map = captions_map or {}\n        # ensure every file has an entry\n        for p in self.files:\n            if p.name not in self.caption_map:\n                self.caption_map[p.name] = \" \"\n        self.tr = transforms.Compose([\n            transforms.Resize((img_size,img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5]*3, [0.5]*3)\n        ])\n    def __len__(self): return len(self.files)\n    def __getitem__(self, idx):\n        p = self.files[idx]\n        img = self.tr(Image.open(p).convert(\"RGB\"))\n        cap = self.caption_map.get(p.name, \" \")\n        return img, cap, str(p)\n\n# --------------------------------------\n# Simple Tokenizer + Text Encoder\n# --------------------------------------\nclass SimpleTokenizer:\n    def __init__(self, max_len=40):\n        self.max_len = max_len\n        self.word2idx = {'<pad>':0,'<unk>':1,'<bos>':2,'<eos>':3}\n        self.idx2word = {v:k for k,v in self.word2idx.items()}\n        self.vocab_size = len(self.word2idx)\n    def build_vocab(self, captions_list, min_freq=1, max_words=20000):\n        freq = {}\n        for c in captions_list:\n            text = c.lower().strip()\n            for ch in ['.',',',';','!','?','\"',\"'\",\"(\",\")\",\":\",\"/\",\"\\\\\"]:\n                text = text.replace(ch,' ')\n            toks = [t for t in text.split() if t!='']\n            for t in toks:\n                freq[t] = freq.get(t,0) + 1\n        items = [w for w,f in sorted(freq.items(), key=lambda x:-x[1]) if f>=min_freq][:max_words]\n        for w in items:\n            if w not in self.word2idx:\n                idx = len(self.word2idx); self.word2idx[w]=idx; self.idx2word[idx]=w\n        self.vocab_size = len(self.word2idx)\n    def encode(self, text):\n        text = text.lower().strip()\n        for ch in ['.',',',';','!','?','\"',\"'\",\"(\",\")\",\":\",\"/\",\"\\\\\"]:\n            text = text.replace(ch,' ')\n        toks = [t for t in text.split() if t!=''][:self.max_len]\n        ids = [self.word2idx.get('<bos>')]\n        for t in toks:\n            ids.append(self.word2idx.get(t, self.word2idx['<unk>']))\n        ids.append(self.word2idx.get('<eos>'))\n        if len(ids) < self.max_len:\n            ids += [self.word2idx['<pad>']] * (self.max_len - len(ids))\n        else:\n            ids = ids[:self.max_len]\n        return torch.tensor(ids, dtype=torch.long)\n\nclass SimpleTextEncoderAvg(nn.Module):\n    def __init__(self, vocab_size, emb_dim=256):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n    def forward(self, token_ids):\n        e = self.emb(token_ids)\n        mask = (token_ids != 0).unsqueeze(-1).float()\n        summed = (e * mask).sum(1)\n        lens = mask.sum(1).clamp(min=1.0)\n        return summed / lens\n\n# --------------------------------------\n# Model / Conditioner / Scheduler\n# --------------------------------------\ndef conv_block(i,o):\n    return nn.Sequential(nn.Conv2d(i,o,3,1,1), nn.GroupNorm(8,o), nn.SiLU())\n\nclass SimpleUNet(nn.Module):\n    def __init__(self, in_ch=3, base=64, text_dim=256, cond_dim=128):\n        super().__init__()\n        self.enc1 = conv_block(in_ch, base)\n        self.enc2 = conv_block(base, base*2)\n        self.enc3 = conv_block(base*2, base*4)\n        self.mid = conv_block(base*4, base*4)\n        self.dec3 = conv_block(base*8, base*4)\n        self.dec2 = conv_block(base*6, base*2)\n        self.dec1 = conv_block(base*3, base)\n        self.out = nn.Conv2d(base, in_ch, 1)\n        self.text_proj = nn.Linear(text_dim, base*4)\n        self.cond_proj = nn.Conv2d(cond_dim, base*4, 1)\n    def forward(self, x, cond_spatial=None, text_emb=None):\n        e1 = self.enc1(x)\n        e2 = self.enc2(F.avg_pool2d(e1,2))\n        e3 = self.enc3(F.avg_pool2d(e2,2))\n        mid = self.mid(F.avg_pool2d(e3,2))\n        if cond_spatial is not None:\n            cs = self.cond_proj(cond_spatial)\n            cs = F.interpolate(cs, size=mid.shape[2:], mode=\"nearest\")\n            mid = mid + cs\n        if text_emb is not None:\n            tproj = self.text_proj(text_emb).view(text_emb.shape[0], -1, 1, 1)\n            mid = mid + tproj\n        d3 = F.interpolate(mid, scale_factor=2)\n        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n        d2 = F.interpolate(d3, scale_factor=2)\n        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n        d1 = F.interpolate(d2, scale_factor=2)\n        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n        return self.out(d1)\n\nclass ConvConditioner(nn.Module):\n    def __init__(self, in_c=3, cond_dim=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_c, cond_dim, 3,1,1),\n            nn.SiLU(),\n            nn.Conv2d(cond_dim, cond_dim, 3,1,1),\n            nn.SiLU()\n        )\n    def forward(self, img):\n        return self.net(img)\n\nclass SimpleScheduler:\n    def __init__(self, timesteps=1000, device=\"cpu\"):\n        betas = torch.linspace(1e-4, 0.02, timesteps, device=device)\n        alphas = 1 - betas\n        self.alphas_cumprod = torch.cumprod(alphas, dim=0)\n        self.timesteps = timesteps\n    def q_sample(self, x0, t, noise):\n        a = self.alphas_cumprod[t].view(-1,1,1,1)\n        return a.sqrt()*x0 + (1-a).sqrt()*noise\n\n@torch.no_grad()\ndef ddim_sample(model, sched, shape, cond_spatial, text_emb, steps, device, scale):\n    x = torch.randn(shape, device=device)\n    idxs = torch.linspace(sched.timesteps-1, 0, steps).long().to(device)\n    for i, t in enumerate(idxs):\n        eps_c = model(x, cond_spatial, text_emb)\n        eps_uc = model(x, cond_spatial, None)\n        eps = eps_uc + scale*(eps_c-eps_uc)\n        a = sched.alphas_cumprod[t].sqrt().view(1,1,1,1)\n        x = (x - (1-a)*eps) / a\n    return x\n\n# --------------------------------------\n# Metric utilities\n# --------------------------------------\ndef noise_prediction_accuracy(pred, true, tol=0.05):\n    diff = (pred - true).abs()\n    correct = (diff <= tol).float()\n    return correct.mean().item()\n\n# --------------------------------------\n# TRAINING LOOP with metrics & val\n# --------------------------------------\ndef train():\n    device = CONFIG[\"device\"]\n    captions_map = load_caption_map(CONFIG[\"captions_json_folder\"])\n    ds = SimpleImageDatasetWithCaptions(CONFIG[\"data_root\"], captions_map, img_size=CONFIG[\"img_size\"])\n    N = len(ds)\n    if N == 0:\n        raise RuntimeError(f\"No images found in {CONFIG['data_root']}\")\n    val_n = max(int(CONFIG['val_fraction'] * N), 1)\n    train_n = N - val_n\n    train_ds, val_ds = random_split(ds, [train_n, val_n])\n    train_dl = DataLoader(train_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2, drop_last=True)\n    val_dl = DataLoader(val_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n    # build tokenizer from dataset captions\n    all_captions = [ds.caption_map.get(Path(p).name, \" \") for p in ds.files]\n    tok = SimpleTokenizer()\n    tok.build_vocab(all_captions)\n    text_enc = SimpleTextEncoderAvg(tok.vocab_size, emb_dim=256).to(device)\n    cond = ConvConditioner(in_c=3, cond_dim=128).to(device)\n    den = SimpleUNet(in_ch=3, base=64, text_dim=256, cond_dim=128).to(device)\n    opt = torch.optim.Adam(list(den.parameters()) + list(cond.parameters()) + list(text_enc.parameters()), lr=CONFIG[\"lr\"])\n    sched = SimpleScheduler(CONFIG[\"timesteps\"], device=device)\n    mse = nn.MSELoss()\n    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n    print(f\"Training on {train_n} images, validating on {val_n} images. Device: {device}\")\n    for epoch in range(1, CONFIG[\"epochs\"]+1):\n        den.train(); cond.train(); text_enc.train()\n        running_loss = 0.0; running_acc = 0.0; seen = 0\n        pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{CONFIG['epochs']}\")\n        for imgs, caps, _ in pbar:\n            imgs = imgs.to(device)\n            B = imgs.shape[0]\n            tokens = torch.stack([tok.encode(c) for c in caps], dim=0).to(device)\n            text_emb = text_enc(tokens)\n            drop = (torch.rand(B, device=device) < CONFIG[\"cf_prob\"]).float().unsqueeze(1)\n            text_emb_cf = text_emb * (1 - drop)\n            cond_in = F.interpolate(imgs, size=(imgs.shape[2]//4, imgs.shape[3]//4), mode='bilinear', align_corners=False)\n            cond_feat = cond(cond_in)\n            t = torch.randint(0, CONFIG[\"timesteps\"], (B,), device=device)\n            noise = torch.randn_like(imgs)\n            zt = sched.q_sample(imgs, t, noise)\n            pred = den(zt, cond_feat, text_emb_cf)\n            loss = mse(pred, noise)\n            opt.zero_grad(); loss.backward(); opt.step()\n            acc = noise_prediction_accuracy(pred.detach(), noise.detach(), tol=0.05)\n            running_loss += float(loss.item()) * B\n            running_acc += acc * B\n            seen += B\n            if seen % (CONFIG[\"batch_size\"] * 2) == 0:\n                pbar.set_postfix(train_loss=(running_loss/seen), train_acc=(running_acc/seen))\n        epoch_train_loss = running_loss / max(1, seen)\n        epoch_train_acc = running_acc / max(1, seen)\n        history['train_loss'].append(epoch_train_loss); history['train_acc'].append(epoch_train_acc)\n        # validation\n        den.eval(); cond.eval(); text_enc.eval()\n        val_loss = 0.0; val_acc = 0.0; vseen = 0\n        with torch.no_grad():\n            for imgs, caps, _ in val_dl:\n                imgs = imgs.to(device)\n                B = imgs.shape[0]\n                tokens = torch.stack([tok.encode(c) for c in caps], dim=0).to(device)\n                text_emb = text_enc(tokens)\n                cond_in = F.interpolate(imgs, size=(imgs.shape[2]//4, imgs.shape[3]//4), mode='bilinear', align_corners=False)\n                cond_feat = cond(cond_in)\n                t = torch.randint(0, CONFIG[\"timesteps\"], (B,), device=device)\n                noise = torch.randn_like(imgs)\n                zt = sched.q_sample(imgs, t, noise)\n                pred = den(zt, cond_feat, text_emb)\n                loss = mse(pred, noise)\n                acc = noise_prediction_accuracy(pred, noise, tol=0.05)\n                val_loss += float(loss.item()) * B\n                val_acc += acc * B\n                vseen += B\n        epoch_val_loss = val_loss / max(1, vseen)\n        epoch_val_acc = val_acc / max(1, vseen)\n        history['val_loss'].append(epoch_val_loss); history['val_acc'].append(epoch_val_acc)\n        print(f\"Epoch {epoch} summary: train_loss={epoch_train_loss:.6f} train_acc={epoch_train_acc:.4f}  val_loss={epoch_val_loss:.6f} val_acc={epoch_val_acc:.4f}\")\n        ck = {\"den\": den.state_dict(), \"cond\": cond.state_dict(), \"text_enc\": text_enc.state_dict(), \"opt\": opt.state_dict(), \"epoch\": epoch}\n        torch.save(ck, os.path.join(CONFIG[\"save_dir\"], f\"ckpt_epoch_{epoch}.pth\"))\n        plot_and_save(history, CONFIG[\"save_dir\"])\n        # save sample from validation\n        try:\n            sample_batch = next(iter(val_dl))\n            imgs0, caps0, _ = sample_batch\n            imgs0 = imgs0.to(device)[:4]; caps0 = caps0[:4]\n            tokens0 = torch.stack([tok.encode(c) for c in caps0], dim=0).to(device)\n            t_emb = text_enc(tokens0)\n            cond_in = F.interpolate(imgs0, size=(imgs0.shape[2]//4, imgs0.shape[3]//4), mode='bilinear', align_corners=False)\n            cond_spatial_sample = cond(cond_in)\n            x = ddim_sample(den, sched, imgs0.shape, cond_spatial_sample, t_emb, steps=CONFIG['sample_steps'], device=device, scale=CONFIG['guidance_scale'])\n            save_image_grid(x.cpu(), os.path.join(CONFIG['save_dir'], \"samples\", f\"epoch_{epoch}_val_sample.png\"))\n        except Exception as e:\n            print(\"Sample save failed:\", e)\n    plot_and_save(history, CONFIG[\"save_dir\"])\n    print(\"Training complete. History saved in\", CONFIG[\"save_dir\"])\n    return history\n\n# --------------------------------------\n# Sampling helpers (same as before)\n# --------------------------------------\n@torch.no_grad()\ndef sample_text2img(prompt=\"a painting of a cat\", ckpt=None, out=None):\n    device = CONFIG[\"device\"]\n    if ckpt is None:\n        ckpt = sorted(Path(CONFIG[\"save_dir\"]).glob(\"ckpt_epoch_*.pth\"))[-1]\n    state = torch.load(str(ckpt), map_location=device)\n    den = SimpleUNet().to(device); den.load_state_dict(state[\"den\"])\n    cond = ConvConditioner().to(device); cond.load_state_dict(state[\"cond\"])\n    text_enc = SimpleTextEncoderAvg(state[\"text_enc\"]['emb.weight'].shape[0], emb_dim=256).to(device)\n    text_enc.load_state_dict(state[\"text_enc\"])\n    tokens = torch.zeros((4,40), dtype=torch.long, device=device)\n    text_emb = text_enc(tokens)\n    cond_spatial = torch.zeros((4,128,CONFIG[\"img_size\"]//4, CONFIG[\"img_size\"]//4), device=device)\n    sched = SimpleScheduler(CONFIG[\"timesteps\"], device=device)\n    x = ddim_sample(den, sched, (4,3,CONFIG[\"img_size\"],CONFIG[\"img_size\"]), cond_spatial, text_emb, steps=CONFIG[\"sample_steps\"], device=device, scale=CONFIG[\"guidance_scale\"])\n    out = out or os.path.join(CONFIG[\"save_dir\"], \"samples\", \"text2img.png\")\n    save_image_grid(x.cpu(), out)\n    print(\"Saved text2img sample to\", out)\n\n@torch.no_grad()\ndef sample_img2img(source_img_path, ckpt=None, out=None, strength=0.6):\n    device = CONFIG[\"device\"]\n    if ckpt is None:\n        ckpt = sorted(Path(CONFIG[\"save_dir\"]).glob(\"ckpt_epoch_*.pth\"))[-1]\n    state = torch.load(str(ckpt), map_location=device)\n    den = SimpleUNet().to(device); den.load_state_dict(state[\"den\"])\n    cond = ConvConditioner().to(device); cond.load_state_dict(state[\"cond\"])\n    text_enc = SimpleTextEncoderAvg(state[\"text_enc\"]['emb.weight'].shape[0], emb_dim=256).to(device)\n    text_enc.load_state_dict(state[\"text_enc\"])\n    tr = transforms.Compose([transforms.Resize((CONFIG[\"img_size\"],CONFIG[\"img_size\"])), transforms.ToTensor(), transforms.Normalize([0.5]*3,[0.5]*3)])\n    img = tr(Image.open(source_img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n    cond_in = F.interpolate(img, size=(img.shape[2]//4, img.shape[3]//4), mode='bilinear', align_corners=False)\n    cond_feat = cond(cond_in)\n    sched = SimpleScheduler(CONFIG[\"timesteps\"], device=device)\n    t_int = int(strength * (sched.timesteps - 1))\n    noise = torch.randn_like(img)\n    zt = sched.q_sample(img, torch.tensor([t_int], device=device), noise)\n    out_img = ddim_sample(den, sched, img.shape, cond_feat, None, steps=CONFIG['sample_steps'], device=device, scale=1.0)\n    out = out or os.path.join(CONFIG[\"save_dir\"], \"samples\", \"img2img.png\")\n    save_image_grid(out_img.cpu(), out)\n    print(\"Saved img2img to\", out)\n\n\nprint(\"Starting training on Kaggle dataset paths...\")\nhistory = train()\nprint(\"Sampling examples...\")\ntry:\n    sample_text2img()\nexcept Exception as e:\n    print(\"Text2Img sampling failed:\", e)\ntry:\n    # try to sample using a real example if present\n    example_path = next(Path(CONFIG['data_root']).rglob(\"*.jpg\"), None)\n    if example_path is not None:\n        sample_img2img(str(example_path))\n    else:\n        print(\"No example image found for img2img sampling.\")\nexcept Exception as e:\n    print(\"Img2Img sampling failed:\", e)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:20:54.505390Z","iopub.execute_input":"2025-12-07T14:20:54.505999Z","iopub.status.idle":"2025-12-07T15:33:20.546192Z","shell.execute_reply.started":"2025-12-07T14:20:54.505973Z","shell.execute_reply":"2025-12-07T15:33:20.545477Z"}},"outputs":[{"name":"stdout","text":"Starting training on Kaggle dataset paths...\nLoaded 2109 captions from /kaggle/input/jssonfile/kaggle/working/caption_jsons_multiGPU\nTraining on 6336 images, validating on 704 images. Device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 792/792 [03:15<00:00,  4.04it/s, train_acc=0.24, train_loss=0.0889] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 summary: train_loss=0.088865 train_acc=0.2396  val_loss=0.053972 val_acc=0.3090\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 792/792 [03:21<00:00,  3.93it/s, train_acc=0.346, train_loss=0.0469]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 summary: train_loss=0.046893 train_acc=0.3459  val_loss=0.042564 val_acc=0.3966\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 792/792 [03:21<00:00,  3.92it/s, train_acc=0.393, train_loss=0.0435]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 summary: train_loss=0.043489 train_acc=0.3926  val_loss=0.041409 val_acc=0.4082\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 792/792 [03:22<00:00,  3.92it/s, train_acc=0.419, train_loss=0.0421]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 summary: train_loss=0.042135 train_acc=0.4186  val_loss=0.047362 val_acc=0.4319\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 792/792 [03:21<00:00,  3.92it/s, train_acc=0.441, train_loss=0.0397]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 summary: train_loss=0.039732 train_acc=0.4412  val_loss=0.033756 val_acc=0.4300\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 792/792 [03:21<00:00,  3.93it/s, train_acc=0.468, train_loss=0.0375]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 summary: train_loss=0.037522 train_acc=0.4679  val_loss=0.039232 val_acc=0.4714\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 792/792 [03:22<00:00,  3.92it/s, train_acc=0.475, train_loss=0.0377]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 summary: train_loss=0.037741 train_acc=0.4752  val_loss=0.031833 val_acc=0.4817\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 792/792 [03:21<00:00,  3.93it/s, train_acc=0.482, train_loss=0.0381]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 summary: train_loss=0.038060 train_acc=0.4818  val_loss=0.031130 val_acc=0.4960\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 792/792 [03:22<00:00,  3.92it/s, train_acc=0.507, train_loss=0.0353]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 summary: train_loss=0.035314 train_acc=0.5067  val_loss=0.035065 val_acc=0.5258\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 792/792 [03:21<00:00,  3.92it/s, train_acc=0.519, train_loss=0.0332]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 summary: train_loss=0.033245 train_acc=0.5185  val_loss=0.033424 val_acc=0.5028\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 792/792 [03:22<00:00,  3.92it/s, train_acc=0.542, train_loss=0.032] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 summary: train_loss=0.032002 train_acc=0.5422  val_loss=0.030329 val_acc=0.5526\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 792/792 [03:21<00:00,  3.92it/s, train_acc=0.538, train_loss=0.0327]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 summary: train_loss=0.032714 train_acc=0.5383  val_loss=0.032893 val_acc=0.5030\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 792/792 [03:22<00:00,  3.92it/s, train_acc=0.538, train_loss=0.034] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 summary: train_loss=0.033979 train_acc=0.5375  val_loss=0.029260 val_acc=0.5750\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 792/792 [03:22<00:00,  3.92it/s, train_acc=0.559, train_loss=0.0324]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 summary: train_loss=0.032378 train_acc=0.5593  val_loss=0.023800 val_acc=0.5855\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 792/792 [03:21<00:00,  3.94it/s, train_acc=0.566, train_loss=0.0305]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 summary: train_loss=0.030522 train_acc=0.5657  val_loss=0.028301 val_acc=0.5750\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 792/792 [03:21<00:00,  3.93it/s, train_acc=0.568, train_loss=0.0305]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 summary: train_loss=0.030539 train_acc=0.5683  val_loss=0.025819 val_acc=0.5989\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 792/792 [03:21<00:00,  3.93it/s, train_acc=0.572, train_loss=0.0313]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 summary: train_loss=0.031271 train_acc=0.5723  val_loss=0.030193 val_acc=0.5778\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 792/792 [03:21<00:00,  3.93it/s, train_acc=0.578, train_loss=0.03]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 summary: train_loss=0.030047 train_acc=0.5783  val_loss=0.028689 val_acc=0.5853\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 792/792 [03:21<00:00,  3.93it/s, train_acc=0.59, train_loss=0.0287] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 summary: train_loss=0.028667 train_acc=0.5896  val_loss=0.027965 val_acc=0.5857\nSaved plots to ./basic_diff_ckpts\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 792/792 [03:21<00:00,  3.93it/s, train_acc=0.591, train_loss=0.0296]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 summary: train_loss=0.029589 train_acc=0.5905  val_loss=0.029487 val_acc=0.5992\nSaved plots to ./basic_diff_ckpts\nSaved plots to ./basic_diff_ckpts\nTraining complete. History saved in ./basic_diff_ckpts\nSampling examples...\nSaved text2img sample to ./basic_diff_ckpts/samples/text2img.png\nSaved img2img to ./basic_diff_ckpts/samples/img2img.png\n","output_type":"stream"}],"execution_count":1}]}